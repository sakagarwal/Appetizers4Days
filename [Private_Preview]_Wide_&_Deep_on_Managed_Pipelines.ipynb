{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakagarwal/Appetizers4Days/blob/master/%5BPrivate_Preview%5D_Wide_%26_Deep_on_Managed_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2018 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ],
      "metadata": {
        "id": "hczXUe2TwMxT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mThXALJl9Yue"
      },
      "source": [
        "# Wide & Deep on Managed Pipelines\n",
        "\n",
        "This notebook showcases how to run the Wide & Deep algorithm with Managed Pipelines on Vertex AI using the built-in pipelines from the SDK.\n",
        "\n",
        "The notebook is organized as follows:\n",
        "\n",
        "- [Setup environment](#setup): Install depdendencies, authenticate, and configure GCP project used in the notebook.\n",
        "- [Define training inputs](#define-training-inputs): Define training inputs such as data location, data split type, feature transformations etc.\n",
        "- [Run a CustomJob](#run-custom-job): Set CustomJob configurations such as machine type and hyperparameters and train a model on Vertex AI Managed Pipelines.\n",
        "- [Run a HyperparameterTuningJob](#run-hyperparameter-tuning-job): Set HyperparameterTuningJob configurations such as machine type and hyperparameters and train a model on Vertex AI Managed Pipelines. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuOonx6suOb7"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "\n",
        "##Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS12JcMdo2qR"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7-SzYTR9bo2"
      },
      "outputs": [],
      "source": [
        "# Depending on the environment, this might throw a\n",
        "# pip dependency resolver error. Please ignore it.\n",
        "!pip3 install -U google-cloud-pipeline-components -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h0cblV-lYMt"
      },
      "outputs": [],
      "source": [
        "# Restart the kernel after pip installs. This can take a minute. \n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G6YmJT1yqkV"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import uuid\n",
        "from google.cloud import aiplatform\n",
        "from google.cloud import storage\n",
        "from google_cloud_pipeline_components.experimental.automl.tabular import utils\n",
        "from typing import Any, Dict, List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfynBI2luKak"
      },
      "source": [
        "### Configure your GCP project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaBOWtyEuMrv"
      },
      "outputs": [],
      "source": [
        "GCP_PROJECT = 'cloud-automl-tables' #@param {type:'string'}\n",
        "GCP_REGION = 'us-central1' #@param {type:'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu0e2TRVxjHb"
      },
      "source": [
        "### Authenticate your GCP account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nJ1VqdTxosw"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  if 'USE_AUTH_EPHEM' in os.environ:\n",
        "    # revert to the old colab authentication module to prevent a bug.\n",
        "    del os.environ['USE_AUTH_EPHEM']\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "  !gcloud config set project {GCP_PROJECT}\n",
        "\n",
        "aiplatform.init(\n",
        "    project=GCP_PROJECT,\n",
        "    location=GCP_REGION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create GCS base path\n",
        "\n",
        "All training related files (TF model checkpoint, TensorBoard file, etc) will be saved to the GCS bucket. The pipeline will not clean up the files since some of them might be useful for you, **please make sure to clean up the files**. For easy cleanup, you can set [GCS bucket level TTL](https://cloud.google.com/storage/docs/lifecycle).\n"
      ],
      "metadata": {
        "id": "OUfwWir9yPNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_BASE_PATH = '' # @param {type:'string'}\n",
        "generate_gcs_base_path = True # @param {type:'boolean'}\n",
        "if generate_gcs_base_path:\n",
        "  bucket_name = f'gs://test-{uuid.uuid4()}'\n",
        "  !gsutil mb -p {GCP_PROJECT} -l {GCP_REGION} {bucket_name}\n",
        "  \n",
        "  # Set GCS bucket object TTL to 7 days\n",
        "  !echo '{\"rule\":[{\"action\": {\"type\": \"Delete\"},\"condition\": {\"age\": 7}}]}' > gcs_lifecycle.tmp\n",
        "  !gsutil lifecycle set gcs_lifecycle.tmp {bucket_name}\n",
        "  !rm gcs_lifecycle.tmp\n",
        "  \n",
        "  GCS_BASE_PATH = bucket_name + '/mp_notebook'\n",
        "  print(f'changed gcs_base_path to {GCS_BASE_PATH} due to generate_gcs_base_path is True')\n"
      ],
      "metadata": {
        "id": "mFZCBHb_yc_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUra_4_vJGn2"
      },
      "source": [
        "### Enable APIs (one-time setup)\n",
        "[Enable the following APIs: Vertex AI APIs, Compute Engine APIs, Cloud Storage and Dataflow.](https://pantheon.corp.google.com/apis/enableflow?apiid=aiplatform.googleapis.com,dataflow,compute_component,storage-component.googleapis.com&mods=-ai_platform_fake_service)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LWH3PRF5o2v"
      },
      "source": [
        "<a name=\"define-training-inputs\"></a>\n",
        "\n",
        "## Define training inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqv5KsLjhZ18"
      },
      "source": [
        "### Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9FPFT8c5oC0"
      },
      "outputs": [],
      "source": [
        "def write_text_to_file(text, filepath):\n",
        "  with tf.io.gfile.GFile(filepath, 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "def get_task_detail(task_details: List[Dict[str, Any]], task_name: str) -> List[Dict[str, Any]]:\n",
        "  \"\"\"Returns the task details for a specified task.\"\"\"\n",
        "  for task_detail in task_details:\n",
        "    if task_detail.task_name == task_name:\n",
        "      return task_detail\n",
        "\n",
        "def get_model_artifacts_path(task_details, task_name):\n",
        "  \"\"\"Returns GCS uri to a file with a path to the training artifacts.\"\"\"\n",
        "  task = get_task_detail(task_details, task_name)\n",
        "  # model_artifact_uri points to a file in GCS that contains the model URI\n",
        "  return task.outputs['unmanaged_container_model'].artifacts[0].uri\n",
        "\n",
        "def get_model_uri(task_details):\n",
        "  \"\"\"Returns a link to the Vertex Model.\"\"\"\n",
        "  task = get_task_detail(task_details, 'model-upload')\n",
        "  # in format https://<location>-aiplatform.googleapis.com/v1/projects/<project_number>/locations/<location>/models/<model_id>\n",
        "  model_id = task.outputs['model'].artifacts[0].uri.split('/')[-1]\n",
        "  return f'https://console.cloud.google.com/vertex-ai/locations/{GCP_REGION}/models/{model_id}?project={GCP_PROJECT}'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training inputs\n",
        "\n",
        "#### The following parameters can be set:\n",
        "\n",
        "##### Pipeline parameters\n",
        "\n",
        "- `project`: The GCP project that runs the pipeline components.\n",
        "- `location`: The GCP region that runs the pipeline components.\n",
        "- `root_dir`: The root GCS directory for the pipeline components.\n",
        "- `target_column`: The target column name.\n",
        "- `prediction_type`: The type of prediction the model is to produce.\n",
        "  'classification' or 'regression'.\n",
        "- `transform_config`: The path to a GCS file containing the transformations to apply.\n",
        "data_source_csv_filenames: The CSV data source.\n",
        "- `data_source_bigquery_table_path`: The BigQuery data source.\n",
        "- `predefined_split_key`: The predefined_split column name.\n",
        "- `timestamp_split_key`: The timestamp_split column name.\n",
        "- `stratified_split_key`: The stratified_split column name.\n",
        "- `training_fraction`: The training fraction.\n",
        "- `validation_fraction`: The validation fraction.\n",
        "- `test_fraction`: The test fraction.\n",
        "- `weight_column`: The weight column name.\n",
        "- `stats_and_example_gen_dataflow_machine_type`: The dataflow machine type for\n",
        "  stats_and_example_gen component.\n",
        "- `stats_and_example_gen_dataflow_max_num_workers`: The max number of Dataflow\n",
        "  workers for stats_and_example_gen component.\n",
        "- `stats_and_example_gen_dataflow_disk_size_gb`: Dataflow worker's disk size in\n",
        "  GB for stats_and_example_gen component.\n",
        "- `transform_dataflow_machine_type`: The dataflow machine type for transform\n",
        "  component.\n",
        "- `transform_dataflow_max_num_workers`: The max number of Dataflow workers for\n",
        "  transform component.\n",
        "- `transform_dataflow_disk_size_gb`: Dataflow worker's disk size in GB for\n",
        "  transform component.\n",
        "- `training_machine_spec`: The machine spec for trainer component. See https://cloud.google.com/compute/docs/machine-types for options.\n",
        "- `training_replica_count`: The replica count for the trainer component.\n",
        "- `run_evaluation`: Whether to run evaluation steps during training.\n",
        "- `evaluation_batch_predict_machine_type`: The prediction server machine type for batch predict components during evaluation.\n",
        "- `evaluation_batch_predict_starting_replica_count`: The initial number of prediction server for batch predict components during evaluation.\n",
        "- `evaluation_batch_predict_max_replica_count`: The max number of prediction server for batch predict components during evaluation.\n",
        "- `evaluation_dataflow_machine_type`: The dataflow machine type for evaluation components.\n",
        "- `evaluation_dataflow_max_num_workers`: The max number of Dataflow workers for evaluation components.\n",
        "- `evaluation_dataflow_disk_size_gb`: Dataflow worker's disk size in GB for evaluation components.\n",
        "- `dataflow_service_account`: Custom service account to run dataflow jobs.\n",
        "- `dataflow_subnetwork`: Dataflow's fully qualified subnetwork name, when empty\n",
        "  the default\n",
        "  subnetwork will be used. Example:\n",
        "    https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications\n",
        "- `dataflow_use_public_ips`: Specifies whether Dataflow workers use public IP\n",
        "  addresses.\n",
        "- `encryption_spec_key_name`: The KMS key name.\n",
        "\n",
        "##### Model hyperparameters\n",
        "- `learning_rate`: The learning rate used by the linear optimizer.\n",
        "- `dnn_learning_rate`: The learning rate for training the deep part of the\n",
        "model.\n",
        "- `optimizer_type`: The type of optimizer to use. Choices are 'adam', 'ftrl' and 'sgd' for the Adam, FTRL, and Gradient Descent Optimizers, respectively.\n",
        "- `max_steps`: Number of steps to run the trainer for.\n",
        "- `max_train_secs`: Amount of time in seconds to run the trainer for.\n",
        "- `l1_regularization_strength`: L1 regularization strength for optimizer_type='ftrl'.\n",
        "- `l2_regularization_strength`: L2 regularization strength for\n",
        "optimizer_type='ftrl'.\n",
        "- `l2_shrinkage_regularization_strength`: L2 shrinkage regularization strength\n",
        "for optimizer_type='ftrl'.\n",
        "- `beta_1`: Beta 1 value for optimizer_type='adam'.\n",
        "- `beta_2`: Beta 2 value for optimizer_type='adam'.\n",
        "- `hidden_units`: Hidden layer sizes to use for DNN feature columns, provided in comma-separated layers.\n",
        "- `use_wide`: If set to True, the categorical columns will be used in the wide\n",
        "part of the DNN model.\n",
        "- `embed_categories`: If set to True, the categorical columns will be used\n",
        "embedded and used in the deep part of the model. Embedding size is the\n",
        "square root of the column cardinality.\n",
        "- `dnn_dropout`: The probability we will drop out a given coordinate.\n",
        "- `dnn_optimizer_type`: The type of optimizer to use for the deep part of the\n",
        "model. Choices are 'adam', 'ftrl' and 'sgd'. for the Adam, FTRL, and\n",
        "Gradient Descent Optimizers, respectively.\n",
        "- `dnn_l1_regularization_strength`: L1 regularization strength for\n",
        "dnn_optimizer_type='ftrl'.\n",
        "- `dnn_l2_regularization_strength`: L2 regularization strength for\n",
        "dnn_optimizer_type='ftrl'.\n",
        "- `dnn_l2_shrinkage_regularization_strength`: L2 shrinkage regularization\n",
        "strength for dnn_optimizer_type='ftrl'.\n",
        "- `dnn_beta_1`: Beta 1 value for dnn_optimizer_type='adam'.\n",
        "- `dnn_beta_2`: Beta 2 value for dnn_optimizer_type='adam'.\n",
        "- `enable_profiler`: Enables profiling and saves a trace during evaluation.\n",
        "- `seed`: Seed to be used for this run.\n",
        "- `eval_steps`: Number of steps to run evaluation for. If not\n",
        "specified or negative, it means run evaluation on the whole validation\n",
        "dataset. If set to 0, it means run evaluation for a fixed number of\n",
        "samples.\n",
        "- `batch_size`: Batch size for training.\n",
        "- `eval_frequency_secs`: Frequency at which evaluation and checkpointing will take place.\n",
        "\n",
        "HyperparameterTuningJob-specific parameters\n",
        "- `study_spec_metrics`: List of dictionaries representing metrics to optimize.\n",
        "The dictionary contains the metric_id, which is reported by the training job, ands the optimization goal of the metric. One of 'minimize' or 'maximize'.\n",
        "- `study_spec_parameters_override`: List of dictionaries representing parameters to\n",
        "optimize. The dictionary key is the parameter_id, which is passed to\n",
        "training job as a command line argument, and the dictionary value is the\n",
        "parameter specification of the metric.\n",
        "- `max_trial_count`: The desired total number of trials.\n",
        "- `parallel_trial_count`: The desired number of trials to run in parallel.\n",
        "- `algorithm`: Which algorithm to train. Specify 'wide_and_deep' here.\n",
        "- `max_failed_trial_count`: The number of failed trials that need to be seen before failing the HyperparameterTuningJob. If set to 0, Vertex AI decides how many trials must fail before the whole job fails.\n",
        "- `study_spec_algorithm`: The search algorithm specified for the study. One of\n",
        "'ALGORITHM_UNSPECIFIED', 'GRID_SEARCH', or 'RANDOM_SEARCH'.- `study_spec_measurement_selection_type`: Which measurement to use if/when the\n",
        "service automatically selects the final measurement from previously reported intermediate measurements. One of 'BEST_MEASUREMENT' or 'LAST_MEASUREMENT'.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "umK2fEtkeQC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Configure dataset"
      ],
      "metadata": {
        "id": "Rn8Clb9Kj_ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV data source\n",
        "csv_filenames = 'gs://automl-tables-us-central1-resources/dataset/safe_driver/train.csv'\n",
        "\n",
        "# BQ data source\n",
        "# When using BQ as data source,\n",
        "# - if BQ is in single-region, GCS root_dir must be in the same single-region.\n",
        "# - if BQ is multi-region, GCS root_dir must be in the same multi-region or the single-region contained by the multi-region.\n",
        "# 'big_query_table_path' = 'bq://cloud-automl-tables-public.datasets.safe_driver'\n",
        "\n",
        "features = ['id', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin']"
      ],
      "metadata": {
        "id": "1j4pgpE6eUN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkXNDwME9qv_"
      },
      "source": [
        "### Configure feature transformation\n",
        "\n",
        "Transformations can be specified using Feature Transform Engine (FTE) specific configurations. In the following, we provide some sample transform configurations to demonstrate FTE's capabilities:\n",
        "- Full auto transformations (i.e., `auto_transform_config`): FTE automatically configure a set of built-in transformations for each input column based on its data statistics. \n",
        "- Fully specified transformations (i.e., `no_auto_transform_config`): All transformations on input columns are explicitly specified with FTE's built-in transformations. Chaining of multiple transformations on a single column is also supported.\n",
        "- Mix of auto and explicit transformations (i.e., `mixed_transform_config`).\n",
        "- Custom transformations (i.e., `transform_config_with_custom_transform`): A mixture of auto and explicit transformations and custom, bring-your-own transform function, where users can define and import their own transform function and use it with FTE's built-in transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNA4f7Dy9oMm"
      },
      "outputs": [],
      "source": [
        "auto_transform_config = {'auto_transforms': features}\n",
        "\n",
        "no_auto_transform_config = {\n",
        "    'transforms': [{\n",
        "        'transform': 'ZScaleTransform',\n",
        "        'input_column_names': ['ps_reg_01']\n",
        "    }, {\n",
        "        'transform': 'ZScaleTransform',\n",
        "        'input_column_names': ['ps_reg_02']\n",
        "    }, {\n",
        "        'transform': 'ZScaleTransform',\n",
        "        'input_column_names': ['ps_reg_03']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_10_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_11_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_12_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['target'],\n",
        "        'output_column_names': ['target']\n",
        "    }]\n",
        "}\n",
        "\n",
        "mixed_transform_config = {\n",
        "    'auto_transforms': ['ps_reg_01', 'ps_reg_02', 'ps_reg_03'],\n",
        "    'transforms': [{\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_10_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_11_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_12_bin']\n",
        "    }]\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "$gsutil cat gs://pvnguyen-us-central1/mp_notebook/custom_transform_fn.py\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "def plus_one_transform(x: tf.SparseTensor) -> tf.SparseTensor:\n",
        "  return tf.SparseTensor(x.indices, tf.add(x.values, 1), x.dense_shape)\n",
        "\"\"\"\n",
        "transform_config_with_custom_transform = {\n",
        "    'auto_transforms': ['ps_reg_02', 'ps_reg_03'],\n",
        "    'modules': [{\n",
        "        'transform': 'PlusOneTransform',\n",
        "        'module_path': 'gs://pvnguyen-us-central1/mp_notebook/custom_transform_fn.py',\n",
        "        'function_name': 'plus_one_transform'\n",
        "    }],\n",
        "    'transforms': [{\n",
        "        'transform': 'CastToFloatTransform',\n",
        "        'input_column_names': ['ps_reg_01'],\n",
        "        'output_column_names': ['ps_reg_01']\n",
        "    },{\n",
        "        'transform': 'PlusOneTransform',\n",
        "        'input_column_names': ['ps_reg_01']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_10_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_11_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_12_bin']\n",
        "    }]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional transformations to try out and their sample configurations:\n",
        "\n",
        "* `DatetimeTransform`:\n",
        "``` python\n",
        "# Outputs columns with granular datetime information (year, month, day, etc.).\n",
        "{\n",
        "    'transform': 'DatetimeTransform',\n",
        "    'input_column_names': ['feature_1'],\n",
        "    'time_format': '%Y-%m-%d'  # time format of input column\n",
        "}\n",
        "```\n",
        "\n",
        "* `LogTransform`:\n",
        "``` python\n",
        "# Outputs a column of the element-wise, natural logarithm of our input.\n",
        "{\n",
        "    'transform': 'LogTransform',\n",
        "    'input_column_names': ['feature_1']\n",
        "}\n",
        "```\n",
        "\n",
        "* `ZScaleTransform`:\n",
        "``` python\n",
        "# Outputs a z-scale normallized input column.\n",
        "{\n",
        "    'transform': 'ZScaleTransform',\n",
        "    'input_column_names': ['feature_1']\n",
        "}\n",
        "```\n",
        "\n",
        "* `NGramTransform`:\n",
        "``` python\n",
        "# Outputs a column containing the vocab lookup incidies of n-grams in our\n",
        "# input.\n",
        "{\n",
        "    'transform': 'NGramTransform',\n",
        "    'input_column_names': ['feature_1'],\n",
        "    'min_ngram_size': 1,  # min number of tokens in our n-gram\n",
        "    'max_ngram_size': 2,  # max number of tokens in our n-gram\n",
        "    'separator': ' '  # seperator between tokens\n",
        "  }\n",
        "```\n",
        "* `ClipTransform`:\n",
        "``` python\n",
        "# Outputs a column where all values < min_value are assigned min_value\n",
        "# and all columns > max_value are assigned max_value.\n",
        "{\n",
        "    'transform': 'ClipTransform',\n",
        "    'input_column_names': ['col1'],\n",
        "    'output_column_names': ['col1_clipped'],\n",
        "    'min_value': 1.,\n",
        "    'max_value': 10.,\n",
        "}\n",
        "```\n",
        "* `MaxAbsScaleTransform`:\n",
        "``` python\n",
        "# Outputs a column where all input elements are divided by abs(max(input)).\n",
        "{\n",
        "    'transform': 'MaxAbsScaleTransform',\n",
        "    'input_column_names': ['col1'],\n",
        "    'output_column_names': ['col1_max_abs_scaled']\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "kP3AQ3ERAH5n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vw19m_vQ6ed"
      },
      "source": [
        "### Setup training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl9mqC7g8PFy"
      },
      "outputs": [],
      "source": [
        "prediction_type = 'classification'\n",
        "target_column = 'target'\n",
        "optimization_objective = 'maximize-au-roc'\n",
        "\n",
        "# Fraction split\n",
        "training_fraction = 0.8\n",
        "validation_fraction = 0.1\n",
        "test_fraction = 0.1\n",
        "\n",
        "# Set feature transformation config\n",
        "transform_config = auto_transform_config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VPC-related config\n",
        "\n",
        "If you need to use a custom Dataflow subnetwork, you can set it through the `dataflow_subnetwork` parameter. The requirements are:\n",
        "1. `dataflow_subnetwork` must be fully qualified subnetwork name.\n",
        "   [[reference](https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications)]\n",
        "1. The following service accounts must have [Compute Network User role](https://cloud.google.com/compute/docs/access/iam#compute.networkUser) assigned on the specified dataflow subnetwork [[reference](https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared)]:\n",
        "    1. Compute Engine default service account: PROJECT_NUMBER-compute@developer.gserviceaccount.com\n",
        "    1. Dataflow service account: service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
        "\n",
        "If your project enable VPC-SC, please make sure:\n",
        "\n",
        "1. The dataflow subnetwork used in VPC-SC is configured properly for Dataflow.\n",
        "   [[reference](https://cloud.google.com/dataflow/docs/guides/routes-firewall)]\n",
        "1. `dataflow_use_public_ips` is set to False.\n",
        "\n"
      ],
      "metadata": {
        "id": "QvkJtxs9L1g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataflow's fully qualified subnetwork name, when empty the default subnetwork will be used.\n",
        "dataflow_subnetwork = ''  #@param {type:'string'}\n",
        "# Specifies whether Dataflow workers use public IP addresses.\n",
        "dataflow_use_public_ips = True  #@param {type:'boolean'}"
      ],
      "metadata": {
        "id": "PsseKKuSf_Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-iXXE14voyR"
      },
      "source": [
        "<a name=\"run-custom-job\"></a>\n",
        "\n",
        "## Customize Wide & Deep CustomJob configuration and create pipeline\n",
        "\n",
        "We will create a Wide & Deep CustomJob pipeline with the following specifications:\n",
        "- Custom training machine type\n",
        "- Specify the following hyperparameters: `learning_rate`, `dnn_learning_rate`, `max_steps`, `max_train_secs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG46cXVueb66"
      },
      "outputs": [],
      "source": [
        "custom_job_root_dir = os.path.join(GCS_BASE_PATH, 'wide_and_deep_custom_job')\n",
        "\n",
        "# max_steps and/or max_train_secs must be set. If both are\n",
        "# specified, training will stop after either condition is met.\n",
        "# By default, max_train_secs is set to -1.\n",
        "max_steps = 1000\n",
        "max_train_secs = -1 \n",
        "\n",
        "learning_rate = 0.01   #  The learning rate used by the linear optimizer.\n",
        "dnn_learning_rate = 0.01  # The learning rate for training the deep part of the model.\n",
        "\n",
        "training_machine_spec =  {\n",
        "    'machine_type': 'c2-standard-16' # Override for TF chief node\n",
        "}\n",
        "\n",
        "transform_config_path = os.path.join(custom_job_root_dir, \"transform_config.json\")\n",
        "write_text_to_file(json.dumps(transform_config), transform_config_path)\n",
        "\n",
        "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
        "# and use another programming language to submit the pipeline.\n",
        "custom_job_template_path, custom_job_parameter_values = utils.get_wide_and_deep_trainer_pipeline_and_parameters(\n",
        "      project=GCP_PROJECT,\n",
        "      location=GCP_REGION,\n",
        "      root_dir=custom_job_root_dir,\n",
        "      max_steps=max_steps,\n",
        "      max_train_secs=max_train_secs,\n",
        "      learning_rate=learning_rate,\n",
        "      dnn_learning_rate=dnn_learning_rate,\n",
        "      target_column=target_column,\n",
        "      prediction_type=prediction_type,\n",
        "      transform_config=transform_config_path,\n",
        "      training_fraction=training_fraction,\n",
        "      validation_fraction=validation_fraction,\n",
        "      test_fraction=test_fraction,\n",
        "      data_source_csv_filenames=csv_filenames,\n",
        "      training_machine_spec=training_machine_spec,\n",
        "      dataflow_use_public_ips=dataflow_use_public_ips,\n",
        "      dataflow_subnetwork=dataflow_subnetwork,\n",
        "      run_evaluation=True,\n",
        ")\n",
        "\n",
        "custom_job_id = f'automl-tabular-wide-and-deep-{uuid.uuid4()}'\n",
        "# More info on parameters PipelineJob accepts:\n",
        "# https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#create_a_pipeline_run \n",
        "custom_job = aiplatform.PipelineJob(\n",
        "    display_name=custom_job_id,\n",
        "    template_path=custom_job_template_path,\n",
        "    job_id=custom_job_id,\n",
        "    pipeline_root=custom_job_root_dir,\n",
        "    parameter_values=custom_job_parameter_values,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "custom_job.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v4tzOoqqTSQ"
      },
      "outputs": [],
      "source": [
        "# Get model URI\n",
        "wide_and_deep_trainer_pipeline_task_details = aiplatform.PipelineJob.get(custom_job_id).gca_resource.job_detail.task_details\n",
        "print('model uri:', get_model_uri(wide_and_deep_trainer_pipeline_task_details))\n",
        "print('model artifacts:', get_model_artifacts_path(wide_and_deep_trainer_pipeline_task_details, 'automl-tabular-wide-and-deep-trainer'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj4PpCRsgx4b"
      },
      "source": [
        "<a name=\"run-hyperparameter-tuning-job\"></a>\n",
        "\n",
        "## Customize Wide & Deep HyperparameterTuningJob configuration and create pipeline\n",
        "\n",
        "We will create a Wide & Deep HyperparameterTuningJob pipeline with the following specifications:\n",
        "- Change training machine type\n",
        "\n",
        "The parameter specs specified in `parameters` can be modified to tune different hyperparameters. The available parameter spec types are [double_value_spec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec#doublevaluespec), [integer_value_spec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec#integervaluespec), [categorical_value_spec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec#integervaluespec), and [discrete_value_spec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/StudySpec#discretevaluespec). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sSj-sZdgx4c"
      },
      "outputs": [],
      "source": [
        "hpt_job_root_dir = os.path.join(GCS_BASE_PATH, 'wide_and_deep_hyperparameter_tuning_job')\n",
        "\n",
        "training_machine_spec =  {\n",
        "    'machine_type': 'c2-standard-16' # Override for TF chief node\n",
        "}\n",
        "\n",
        "transform_config_path = os.path.join(hpt_job_root_dir, \"transform_config.json\")\n",
        "write_text_to_file(json.dumps(transform_config), transform_config_path)\n",
        "\n",
        "metrics = [{'metric_id': 'loss', 'goal': 'MINIMIZE'}]\n",
        "parameters = utils.get_wide_and_deep_study_spec_parameters_override()\n",
        "# max_steps and/or max_train_secs must be set. If both are\n",
        "# specified, training will stop after either condition is met.\n",
        "# By default, max_train_secs is set to -1 and max_steps is set to\n",
        "# an appropriate range given dataset_size and training budget.\n",
        "\n",
        "# If your system does not use Python, you can save the JSON file (`template_path`),\n",
        "# and use another programming language to submit the pipeline.\n",
        "hpt_job_template_path, hpt_job_parameter_values = utils.get_builtin_algorithm_hyperparameter_tuning_job_pipeline_and_parameters(\n",
        "      project=GCP_PROJECT,\n",
        "      location=GCP_REGION,\n",
        "      root_dir=hpt_job_root_dir,\n",
        "      algorithm='wide_and_deep',\n",
        "      target_column=target_column,\n",
        "      prediction_type=prediction_type,\n",
        "      transform_config=transform_config_path,\n",
        "      training_fraction=training_fraction,\n",
        "      validation_fraction=validation_fraction,\n",
        "      test_fraction=test_fraction,\n",
        "      data_source_csv_filenames=csv_filenames,\n",
        "      study_spec_metrics=metrics,\n",
        "      study_spec_parameters_override=parameters,\n",
        "      max_trial_count=1,\n",
        "      parallel_trial_count=1,\n",
        "      max_failed_trial_count=0,\n",
        "      training_machine_spec=training_machine_spec,\n",
        "      dataflow_use_public_ips=dataflow_use_public_ips,\n",
        "      dataflow_subnetwork=dataflow_subnetwork,\n",
        "      run_evaluation=True,\n",
        ")\n",
        "\n",
        "hpt_job_id = f'automl-tabular-wide-and-deep-hpt-{uuid.uuid4()}'\n",
        "# More info on parameters PipelineJob accepts:\n",
        "# https://cloud.google.com/vertex-ai/docs/pipelines/run-pipeline#create_a_pipeline_run \n",
        "hpt_job = aiplatform.PipelineJob(\n",
        "    display_name=hpt_job_id,\n",
        "    template_path=hpt_job_template_path,\n",
        "    job_id=hpt_job_id,\n",
        "    pipeline_root=hpt_job_root_dir,\n",
        "    parameter_values=hpt_job_parameter_values,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "hpt_job.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLcC4Hqagx4d"
      },
      "outputs": [],
      "source": [
        "# Get model URI\n",
        "wide_and_deep_hpt_pipeline_task_details = aiplatform.PipelineJob.get(hpt_job_id).gca_resource.job_detail.task_details\n",
        "print('model uri:', get_model_uri(wide_and_deep_hpt_pipeline_task_details))\n",
        "print('model artifacts:', get_model_artifacts_path(wide_and_deep_hpt_pipeline_task_details, 'get-best-hyperparameter-tuning-job-trial'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClqGTDbc8uxG"
      },
      "source": [
        "## Notes about service account and permission\n",
        "\n",
        "**By default no configuration is required**, if you run into any permission related issue, please make sure the service accounts above have the required roles:\n",
        "\n",
        "|Service account email|Description|Roles|\n",
        "|---|---|---|\n",
        "|PROJECT_NUMBER-compute@developer.gserviceaccount.com|Compute Engine default service account|Dataflow Admin, Dataflow Worker, Storage Admin, Service Account User, BigQuery Admin, Vertex AI User|\n",
        "|service-PROJECT_NUMBER@gcp-sa-aiplatform.iam.gserviceaccount.com|AI Platform Service Agent|Vertex AI Service Agent, Dataflow Admin, Service Account Token Creator|\n",
        "\n",
        "\n",
        "1. Goto https://console.cloud.google.com/iam-admin/iam.\n",
        "2. Check the 'Include Google-provided role grants' checkbox.\n",
        "3. Find the above emails.\n",
        "4. Grant the corresponding roles.\n",
        "\n",
        "### Using data source from a different project\n",
        "- For the BQ data source, grant both service accounts the 'BigQuery Data Viewer' role.\n",
        "- For the CSV data source, grant both service accounts the 'Storage Object Viewer' role.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "[Private Preview] Wide & Deep on Managed Pipelines",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}