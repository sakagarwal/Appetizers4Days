{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakagarwal/Appetizers4Days/blob/master/%5BPrivate_Preview%5D_Feature_Transform_Engine_with_XGBoost_%26_SKLearn_Trainer_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Vertex AI Tabular Feature Transform Engine (or FTE for short) allows users to transform their structured data's input features in a consistent fashion across training and prediction.\n",
        "\n",
        "In this notebook, we demonstrate the basic functionalities of FTE and how to use FTE to apply row-level transformations on raw input and integrate with custom trainers (e.g., Xgboost, sklearn-based trainers) during training and prediction.\n",
        "\n",
        "The notebook is organized as follows:\n",
        "- [Setup environment](#setup): Install depdendencies, authenticate, and configure GCP project used in the notebook.\n",
        "- [Configure transformations](#configure-transforms): Configure feature transformations using FTE's builtin transformations or your own custom transform functions.\n",
        "- [Train new model](#train-new-model): Define a training pipeline that use FTE for feature transformation and custom trainer (e.g., Xgboost, scikit-learn) for training a new model. Run the training pipeline on Vertex AI Pipelines.\n",
        "- [Test the trained model](#online-prediction): Deploy the newly trained model using FTE-supported prediciton server on Vertex AI and perform online (or batch) prediction with raw input seamlessly."
      ],
      "metadata": {
        "id": "zNQjgpCjiBVo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcxIEgvF6Qwv"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuOonx6suOb7"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZRvOKd3pcyr"
      },
      "outputs": [],
      "source": [
        "# Depending on the environment, this might throw a\n",
        "# pip dependency resolver error. Please ignore it.\n",
        "!pip3 install -U google-cloud-aiplatform -q\n",
        "!pip3 install -U google-cloud-pipeline-components --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h0cblV-lYMt"
      },
      "outputs": [],
      "source": [
        "# Restart the kernel after pip installs. This can take a minute. \n",
        "import IPython\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G6YmJT1yqkV"
      },
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import tempfile\n",
        "import uuid\n",
        "from google.cloud import aiplatform\n",
        "from google_cloud_pipeline_components.experimental.automl.tabular import utils\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google_cloud_pipeline_components.experimental.automl import tabular\n",
        "from google_cloud_pipeline_components.aiplatform import ModelUploadOp\n",
        "from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp\n",
        "from google_cloud_pipeline_components.types import artifact_types\n",
        "from kfp.v2.components import importer_node\n",
        "from kfp import components\n",
        "from kfp.v2 import compiler\n",
        "from kfp.v2 import dsl\n",
        "from kfp.v2.dsl import component\n",
        "from kfp.v2.dsl import Dataset\n",
        "from kfp.v2.dsl import Input\n",
        "from kfp.v2.dsl import InputPath\n",
        "from kfp.v2.dsl import Model\n",
        "from kfp.v2.dsl import Metrics\n",
        "from kfp.v2.dsl import Output\n",
        "from kfp.v2.dsl import OutputPath\n",
        "from kfp.v2.dsl import Artifact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfynBI2luKak"
      },
      "source": [
        "### Configure your GCP project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaBOWtyEuMrv"
      },
      "outputs": [],
      "source": [
        "GCP_PROJECT = \"cloud-automl-tables\" #@param {type:\"string\"}\n",
        "GCP_REGION = \"us-east1\" #@param {type:\"string\"}\n",
        "\n",
        "# All training related files (TF model checkpoint, saved_model, etc) will\n",
        "# be saved to this GCS bucket. The pipeline will not clean up the files since\n",
        "# some of them might be useful for you, please make sure to clean up them if\n",
        "# needed.\n",
        "GCP_BASE_PATH = \"gs://pvnguyen-us-central1/mp_notebook\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu0e2TRVxjHb"
      },
      "source": [
        "### Authenticate your GCP account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nJ1VqdTxosw"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in sys.modules:\n",
        "  if 'USE_AUTH_EPHEM' in os.environ:\n",
        "    # revert to the old colab authentication module to prevent a bug.\n",
        "    del os.environ['USE_AUTH_EPHEM']\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "  !gcloud config set project {GCP_PROJECT}\n",
        "\n",
        "aiplatform.init(\n",
        "    project=GCP_PROJECT,\n",
        "    location=GCP_REGION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUra_4_vJGn2"
      },
      "source": [
        "### Enable APIs (one time setup)\n",
        "\n",
        "This is not required if the APIs are already enabled.\n",
        "\n",
        "The code below enables the following APIs:\n",
        "- Vertex AI\n",
        "- Dataflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne1TuD5pJJDA"
      },
      "outputs": [],
      "source": [
        "!gcloud services enable aiplatform.googleapis.com \n",
        "!gcloud services enable dataflow.googleapis.com "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"configure-transforms\"></a>\n",
        "## Feature transformation\n",
        "\n",
        "Before configuring feature transformation, let's define some helper functions that will be used throughout the notebook and configure the input dataset:"
      ],
      "metadata": {
        "id": "N0kqtCxvinL1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LWH3PRF5o2v"
      },
      "source": [
        "### Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9FPFT8c5oC0"
      },
      "outputs": [],
      "source": [
        "def get_task_detail(task_details: List[Dict[str, Any]], task_name: str) -> List[Dict[str, Any]]:\n",
        "  for task_detail in task_details:\n",
        "    if task_detail.task_name == task_name:\n",
        "      return task_detail\n",
        "\n",
        "def get_model_name(custom_job_id):\n",
        "  pipeline_task_details = aiplatform.PipelineJob.get(custom_job_id).gca_resource.job_detail.task_details\n",
        "  upload_task_details = get_task_detail(pipeline_task_details, 'model-upload')\n",
        "  return upload_task_details.outputs['model'].artifacts[0].metadata['resourceName']\n",
        "\n",
        "def write_text_to_file(text, filepath):\n",
        "  with tf.io.gfile.GFile(filepath, 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "def write_instances_to_jsonl(instances, filepath):\n",
        "  with tf.io.gfile.GFile(filepath, \"w\") as f:\n",
        "    for instance in instances:\n",
        "        f.write(json.dumps(instance) + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure dataset"
      ],
      "metadata": {
        "id": "FOG5bJNMis2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV data source\n",
        "csv_filenames = 'gs://automl-tables-us-central1-resources/dataset/safe_driver_small.csv'\n",
        "\n",
        "features = ['id', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat', 'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03', 'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat', 'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11', 'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01', 'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin']"
      ],
      "metadata": {
        "id": "QKEj0ohriwd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-depdZK0D8S4"
      },
      "source": [
        "### Configure feature transformation\n",
        "\n",
        "Transformations can be specified using FTE specific configurations. In the following, we provide some sample transform configurations to demonstrate FTE's capabilities:\n",
        "- Full auto transformations (i.e., `auto_transform_config`): FTE automatically configure a set of built-in transformations for each input column based on its data statistics. \n",
        "- Fully specified transformations (i.e., `no_auto_transform_config`): All transformations on input columns are explicitly specified with FTE's built-in transformations. Chaining of multiple transformations on a single column is also supported.\n",
        "- Mix of auto and explicit transformations (i.e., `mixed_transform_config`).\n",
        "- Custom transformations (i.e., `transform_config_with_custom_transform`): A mixture of auto and explicit transformations and custom, bring-your-own transform function, where users can define and import their own transform function and use it with FTE's built-in transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc2PD7fPEBdt"
      },
      "outputs": [],
      "source": [
        "auto_transform_config = {'auto_transforms': features}\n",
        "\n",
        "no_auto_transform_config = {\n",
        "    'transforms': [{\n",
        "        'transform': 'ZScaleTransform',\n",
        "        'input_column_names': ['ps_reg_01']\n",
        "    }, {\n",
        "        'transform': 'ZScaleTransform',\n",
        "        'input_column_names': ['ps_reg_02']\n",
        "    }, {\n",
        "        'transform': 'ZScaleTransform',\n",
        "        'input_column_names': ['ps_reg_03']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_10_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_11_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_12_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['target'],\n",
        "        'output_column_names': ['target']\n",
        "    }]\n",
        "}\n",
        "\n",
        "mixed_transform_config = {\n",
        "    'auto_transforms': ['ps_reg_01', 'ps_reg_02', 'ps_reg_03'],\n",
        "    'transforms': [{\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_10_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_11_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_12_bin']\n",
        "    }]\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "$gsutil cat gs://pvnguyen-us-central1/mp_notebook/custom_transform_fn.py\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "\n",
        "def plus_one_transform(x: tf.SparseTensor) -> tf.SparseTensor:\n",
        "  return tf.SparseTensor(x.indices, tf.add(x.values, 1), x.dense_shape)\n",
        "\"\"\"\n",
        "transform_config_with_custom_transform = {\n",
        "    'auto_transforms': ['ps_reg_02', 'ps_reg_03'],\n",
        "    'modules': [{\n",
        "        'transform': 'PlusOneTransform',\n",
        "        'module_path': 'gs://pvnguyen-us-central1/mp_notebook/custom_transform_fn.py',\n",
        "        'function_name': 'plus_one_transform'\n",
        "    }],\n",
        "    'transforms': [{\n",
        "        'transform': 'CastToFloatTransform',\n",
        "        'input_column_names': ['ps_reg_01'],\n",
        "        'output_column_names': ['ps_reg_01']\n",
        "    },{\n",
        "        'transform': 'PlusOneTransform',\n",
        "        'input_column_names': ['ps_reg_01']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_10_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_11_bin']\n",
        "    }, {\n",
        "        'transform': 'VocabularyTransform',\n",
        "        'input_column_names': ['ps_ind_12_bin']\n",
        "    }]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNCwvrtkPZIy"
      },
      "source": [
        "Or customize your own tranform function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC4TbiBJPd_n"
      },
      "outputs": [],
      "source": [
        "func_str = \"\"\"\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def plus_one_transform(x: tf.SparseTensor) -> tf.SparseTensor:\n",
        "  return tf.SparseTensor(x.indices, tf.add(x.values, 1), x.dense_shape)\n",
        "\"\"\"\n",
        "\n",
        "write_text_to_file(func_str, os.path.join(GCP_BASE_PATH, 'custom_transform_fn.py'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnMHbAqn_5ZW"
      },
      "source": [
        "Additional transformations to try out and their sample configurations:\n",
        "\n",
        "* `DatetimeTransform`:\n",
        "``` python\n",
        "# Outputs columns with granular datetime information (year, month, day, etc.).\n",
        "{\n",
        "    'transform': 'DatetimeTransform',\n",
        "    'input_column_names': ['feature_1'],\n",
        "    'time_format': '%Y-%m-%d'  # time format of input column\n",
        "}\n",
        "```\n",
        "\n",
        "* `LogTransform`:\n",
        "``` python\n",
        "# Outputs a column of the element-wise, natural logarithm of our input.\n",
        "{\n",
        "    'transform': 'LogTransform',\n",
        "    'input_column_names': ['feature_1']\n",
        "}\n",
        "```\n",
        "\n",
        "* `ZScaleTransform`:\n",
        "``` python\n",
        "# Outputs a z-scale normallized input column.\n",
        "{\n",
        "    'transform': 'ZScaleTransform',\n",
        "    'input_column_names': ['feature_1']\n",
        "}\n",
        "```\n",
        "\n",
        "* `NGramTransform`:\n",
        "``` python\n",
        "# Outputs a column containing the vocab lookup incidies of n-grams in our\n",
        "# input.\n",
        "{\n",
        "    'transform': 'NGramTransform',\n",
        "    'input_column_names': ['feature_1'],\n",
        "    'min_ngram_size': 1,  # min number of tokens in our n-gram\n",
        "    'max_ngram_size': 2,  # max number of tokens in our n-gram\n",
        "    'separator': ' '  # seperator between tokens\n",
        "  }\n",
        "```\n",
        "* `ClipTransform`:\n",
        "``` python\n",
        "# Outputs a column where all values < min_value are assigned min_value\n",
        "# and all columns > max_value are assigned max_value.\n",
        "{\n",
        "    'transform': 'ClipTransform',\n",
        "    'input_column_names': ['col1'],\n",
        "    'output_column_names': ['col1_clipped'],\n",
        "    'min_value': 1.,\n",
        "    'max_value': 10.,\n",
        "}\n",
        "```\n",
        "* `MaxAbsScaleTransform`:\n",
        "``` python\n",
        "# Outputs a column where all input elements are divided by abs(max(input)).\n",
        "{\n",
        "    'transform': 'MaxAbsScaleTransform',\n",
        "    'input_column_names': ['col1'],\n",
        "    'output_column_names': ['col1_max_abs_scaled']\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvksakJM6bEG"
      },
      "source": [
        "<a name=\"train-new-model\"></a>\n",
        "## Train new model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctzAUGD7X5ri"
      },
      "source": [
        "### Define training pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKhNx-7m6g9J"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    base_image='us-docker.pkg.dev/vertex-ai/automl-tabular/custom-trainer:20220629_2125_RC00'\n",
        ")\n",
        "def custom_train(\n",
        "    materialized_train_data: InputPath('MaterializedSplit'),\n",
        "    materialized_test_data: InputPath('MaterializedSplit'),\n",
        "    training_schema: InputPath('TrainingSchema'),\n",
        "    transform_output: InputPath('TransformOutput'),\n",
        "    model_type: str,\n",
        "    target_column: str, \n",
        "    metrics: Output[Metrics],\n",
        "    model: Output[Model]\n",
        ") -> str:\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import pandas_tfrecords\n",
        "    import tempfile\n",
        "    import yaml\n",
        "    import tensorflow as tf\n",
        "    from google.cloud import storage\n",
        "    from joblib import dump\n",
        "    from pathlib import Path\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from xgboost import XGBClassifier\n",
        "\n",
        "    def get_bucket_name_and_path(uri):\n",
        "      no_prefix_uri = uri[len('gs://'):]\n",
        "      splits = no_prefix_uri.split('/')\n",
        "      return splits[0], '/'.join(splits[1:])\n",
        "\n",
        "    def download_gcs_directory(source_uri, dest_dir):\n",
        "      storage_client = storage.Client()\n",
        "\n",
        "      source_bucket_name, path = get_bucket_name_and_path(source_uri)\n",
        "      source_bucket = storage_client.get_bucket(source_bucket_name)\n",
        "\n",
        "      for blob in source_bucket.list_blobs(prefix=path):\n",
        "        if blob.name.endswith(\"/\"):\n",
        "            continue\n",
        "        directory = \"/\".join(blob.name.split(\"/\")[0:-1])\n",
        "        directory = directory.replace(path, dest_dir)\n",
        "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
        "        destination_uri = os.path.join(directory, blob.name.split(\"/\")[-1]) \n",
        "        blob.download_to_filename(destination_uri)\n",
        "    \n",
        "    with open(materialized_train_data, 'r') as f:\n",
        "      train_file_pattern = f.read()\n",
        "    train_paths = tf.io.gfile.glob(train_file_pattern)\n",
        "    copy_paths = []\n",
        "    for fn in train_paths:\n",
        "      tmp_path = tempfile.mkstemp()[1]\n",
        "      tf.io.gfile.copy(fn, tmp_path, overwrite=True)\n",
        "      copy_paths.append(tmp_path)\n",
        "    \n",
        "    df_train = pandas_tfrecords.tfrecords_to_pandas(copy_paths, schema=None, \n",
        "                                                    compression_type='GZIP', cast=True)\n",
        "\n",
        "    y_train = df_train.pop(target_column).tolist()\n",
        "    df_train = df_train.reindex(sorted(df_train.columns), axis=1)\n",
        "    x_train = df_train.values.tolist()\n",
        "\n",
        "    with open(materialized_test_data, 'r') as f:\n",
        "      test_file_pattern = f.read()\n",
        "    test_paths = tf.io.gfile.glob(test_file_pattern)\n",
        "    copy_paths = []\n",
        "    for fn in test_paths:\n",
        "      tmp_path = tempfile.mkstemp()[1]\n",
        "      tf.io.gfile.copy(fn, tmp_path, overwrite=True)\n",
        "      copy_paths.append(tmp_path)\n",
        "    \n",
        "    df_test = pandas_tfrecords.tfrecords_to_pandas(copy_paths, schema=None, \n",
        "                                                   compression_type='GZIP', cast=True)\n",
        "\n",
        "    y_test = df_test.pop(target_column).tolist()\n",
        "    df_test = df_test.reindex(sorted(df_test.columns), axis=1)\n",
        "    x_test = df_test.values.tolist()\n",
        "\n",
        "    model_artifact_path = \"/\".join(model.path.split(\"/\")[0:-1])\n",
        "    if model_type == 'xgboost':\n",
        "      xgbmodel = XGBClassifier(n_jobs=10, n_estimators=30)\n",
        "      xgbmodel.fit(x_train, y_train)\n",
        "\n",
        "      y_pred = xgbmodel.predict(x_test)\n",
        "      predictions = [round(value) for value in y_pred]\n",
        "\n",
        "      accuracy = accuracy_score(y_test, predictions)\n",
        "      metrics.log_metric(\"accuracy\",(accuracy * 100.0))\n",
        "      metrics.log_metric(\"framework\", \"XGBoost\")\n",
        "      xgbmodel.save_model(model.path + f\".bst\")\n",
        "\n",
        "    elif model_type == 'sklearn':\n",
        "      skmodel = DecisionTreeClassifier()\n",
        "      skmodel.fit(x_train,y_train)\n",
        "      score = skmodel.score(x_test,y_test)\n",
        "      metrics.log_metric(\"accuracy\",(score * 100.0))\n",
        "      metrics.log_metric(\"framework\", \"Scikit Learn\")\n",
        "      dump(skmodel, model.path + \".joblib\")\n",
        "\n",
        "    with open(transform_output, 'r') as f:\n",
        "      transform_output_path = f.read()\n",
        "    local_transform_artifact_dir = os.path.join(model_artifact_path, \"transform\")\n",
        "    download_gcs_directory(transform_output_path, local_transform_artifact_dir)\n",
        "\n",
        "    training_schema_filepath = os.path.join(model_artifact_path, \"training_schema.yaml\")\n",
        "    tf.io.gfile.copy(training_schema, training_schema_filepath)\n",
        "    return \"/\".join(model.uri.split(\"/\")[0:-1])\n",
        "\n",
        "\n",
        "@dsl.pipeline(name='training-pipeline-with-fte')\n",
        "def training_pipeline_with_fte(\n",
        "    project: str,\n",
        "    location: str,\n",
        "    root_dir: str,\n",
        "    target_column_name: str,\n",
        "    prediction_type: str,\n",
        "    transform_config_path: str,\n",
        "    model_type: str,\n",
        "    training_fraction: float,\n",
        "    validation_fraction: float,\n",
        "    test_fraction: float,\n",
        "    csv_filenames: str,\n",
        "    weight_column_name: str = '',\n",
        "    dataflow_use_public_ips: bool = False,\n",
        "    dataflow_subnetwork: str = ''):\n",
        "  \"\"\"Defines training pipeline with feature transform engine component.\"\"\"\n",
        "  import json\n",
        "  import os\n",
        "\n",
        "  stats_and_example_gen_task = tabular.StatsAndExampleGenOp(\n",
        "      project=project,\n",
        "      location=location,\n",
        "      target_column_name=target_column_name,\n",
        "      weight_column_name=weight_column_name,\n",
        "      prediction_type=prediction_type,\n",
        "      transformations='[]',\n",
        "      training_fraction=training_fraction,\n",
        "      validation_fraction=validation_fraction,\n",
        "      test_fraction=test_fraction,\n",
        "      data_source_csv_filenames=csv_filenames,\n",
        "      request_type='COLUMN_STATS_ONLY_NO_TRANSFORM',\n",
        "      dataflow_use_public_ips=dataflow_use_public_ips,\n",
        "      dataflow_subnetwork=dataflow_subnetwork,\n",
        "      root_dir=root_dir)\n",
        "  \n",
        "  #  pylint: disable=no-value-for-parameter\n",
        "  generate_analyze_and_transform_data_task = tabular.GenerateAnalyzeAndTransformDataOp(\n",
        "      train_split=stats_and_example_gen_task.outputs['train_split'],\n",
        "      eval_split=stats_and_example_gen_task.outputs['eval_split'],\n",
        "      test_split=stats_and_example_gen_task.outputs['test_split'])\n",
        "\n",
        "  fte_transform_configure_task = tabular.TransformConfigurationPlannerOp(\n",
        "      project=project,\n",
        "      location=location,\n",
        "      root_dir=root_dir,\n",
        "      analyze_data=generate_analyze_and_transform_data_task\n",
        "      .outputs['analyze_data'],\n",
        "      prediction_type=prediction_type,\n",
        "      target_column=target_column_name,\n",
        "      weight_column=weight_column_name,\n",
        "      transform_config=transform_config_path,\n",
        "      dataset_stats=stats_and_example_gen_task.outputs['dataset_stats'])\n",
        "\n",
        "  fte_task = tabular.FeatureTransformEngineOp(\n",
        "      project=project,\n",
        "      location=location,\n",
        "      root_dir=root_dir,\n",
        "      analyze_data=generate_analyze_and_transform_data_task\n",
        "      .outputs['analyze_data'],\n",
        "      transform_data=generate_analyze_and_transform_data_task\n",
        "      .outputs['transform_data'],\n",
        "      transform_config=fte_transform_configure_task\n",
        "      .outputs['fte_transform_configuration_path'],\n",
        "      dataflow_use_public_ips=dataflow_use_public_ips,\n",
        "      dataflow_subnetwork=dataflow_subnetwork\n",
        "  ).set_cpu_limit('8').set_memory_limit('30G')\n",
        "  \n",
        "  split_materialized_data_task = tabular.SplitMaterializedDataOp(\n",
        "        fte_task.outputs['materialized_data'])\n",
        "  \n",
        "  train_task = custom_train(\n",
        "      materialized_train_data=split_materialized_data_task.outputs['materialized_train_split'],\n",
        "      materialized_test_data=split_materialized_data_task.outputs['materialized_test_split'],\n",
        "      training_schema=fte_transform_configure_task.outputs['training_schema'],\n",
        "      transform_output=fte_task.outputs['transform_output'],\n",
        "      target_column=target_column_name, model_type=model_type).set_cpu_limit('16').set_memory_limit('128G')\n",
        "      \n",
        "  importer_spec = importer_node.importer(\n",
        "      artifact_uri=train_task.outputs['output'],\n",
        "      artifact_class=artifact_types.UnmanagedContainerModel,\n",
        "      metadata={\n",
        "          \"containerSpec\": {\n",
        "              \"imageUri\": \"gcr.io/cloud-automl-tables/fte-prediction-server:2022_06_27_07_08_41\",\n",
        "              \"predictRoute\": \"/predict\",\n",
        "              \"healthRoute\": \"/health\"\n",
        "          }\n",
        "      })\n",
        "\n",
        "  ModelUploadOp(\n",
        "      project=project,\n",
        "      location=location,\n",
        "      display_name='custom-trained-model-with-fte-{{$.pipeline_job_uuid}}-{{$.pipeline_task_uuid}}',\n",
        "      unmanaged_container_model=importer_spec.outputs['artifact']\n",
        "  )\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZsm8wrS6FBc"
      },
      "source": [
        "### Setup training configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl9mqC7g8PFy"
      },
      "outputs": [],
      "source": [
        "prediction_type = 'classification'\n",
        "target_column = 'target'\n",
        "model_type = 'xgboost'  # sklearn or xgboost\n",
        "\n",
        "# Fraction split\n",
        "training_fraction = 0.8\n",
        "validation_fraction = 0.1\n",
        "test_fraction = 0.1\n",
        "\n",
        "# Set feature transformation config\n",
        "transform_config = auto_transform_config\n",
        "\n",
        "# VPC-SC related config. If your project does not enable VPC-SC, you likely\n",
        "# don't need to change the values below. If your project enables VPC-SC, please\n",
        "# make sure:\n",
        "# 1) The VPC is configured properly for Dataflow.\n",
        "#    Reference: https://cloud.google.com/dataflow/docs/guides/routes-firewall\n",
        "# 2) Set dataflow_use_public_ips to False.\n",
        "# 3) Set the correct fully qualified subnetwork name for dataflow_subnetwork.\n",
        "#    Reference: https://cloud.google.com/dataflow/docs/guides/specifying-networks#example_network_and_subnetwork_specifications\n",
        "\n",
        "# Specifies whether Dataflow workers use public IP addresses.\n",
        "dataflow_use_public_ips = True\n",
        "# Dataflow's fully qualified subnetwork name (e.g., \"regions/us-central1/subnetworks/default\"), when empty the default subnetwork will be used.\n",
        "dataflow_subnetwork = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQZGHR0wUGAM"
      },
      "source": [
        "### Create and run training pipeline\n",
        "\n",
        "By default, the [service account](https://cloud.google.com/iam/docs/service-accounts) used for your pipeline run is your [default compute engine service account](https://cloud.google.com/compute/docs/access/service-accounts#default_service_account). However, you might want to run pipelines with permissions to access different roles than those configured for your default SA (e.g. perhaps using a more restricted set of permissions).\n",
        "\n",
        "User can set custom service account via `service_account` option when triggering a pipeline run with `custom_job.run()` belows.\n",
        "\n",
        "Please ensure that the service account used to run the pipeline has sufficient permissions to access GCS directory to read/write data, and launch Dataflow jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW_T4efo7Qv9"
      },
      "outputs": [],
      "source": [
        "job_id = str(uuid.uuid4())\n",
        "root_dir = GCP_BASE_PATH + '/' + job_id\n",
        "\n",
        "transform_config_path = os.path.join(root_dir, \"transform_config.json\")\n",
        "write_text_to_file(json.dumps(transform_config), transform_config_path)\n",
        "\n",
        "pipeline_definition_path = os.path.join(tempfile.mkdtemp(), 'pipeline.json')\n",
        "compiler.Compiler().compile(training_pipeline_with_fte, \n",
        "                            pipeline_definition_path)\n",
        "parameter_values = {\n",
        "    'project': GCP_PROJECT,\n",
        "    'location': GCP_REGION,\n",
        "    'root_dir': root_dir,\n",
        "    'target_column_name': target_column,\n",
        "    'prediction_type': prediction_type,\n",
        "    'transform_config_path': transform_config_path,\n",
        "    'model_type': model_type, \n",
        "    'training_fraction': training_fraction,\n",
        "    'validation_fraction': validation_fraction,\n",
        "    'test_fraction': test_fraction,\n",
        "    'csv_filenames': csv_filenames,\n",
        "    'dataflow_use_public_ips': dataflow_use_public_ips,\n",
        "    'dataflow_subnetwork': dataflow_subnetwork,\n",
        "}\n",
        "\n",
        "custom_job_id = 'feature-transform-engine-with-non-tf-trainer-{}'.format(job_id)\n",
        "custom_job = aiplatform.PipelineJob(\n",
        "    display_name=custom_job_id,\n",
        "    template_path=pipeline_definition_path,\n",
        "    job_id=custom_job_id,\n",
        "    pipeline_root=root_dir,\n",
        "    parameter_values=parameter_values,\n",
        "    enable_caching=False,\n",
        ")\n",
        "\n",
        "custom_job.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNnbPXgo35QT"
      },
      "source": [
        "<a name=\"online-prediction\"></a>\n",
        "## Test the trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqP4xAIho44a"
      },
      "source": [
        "### Deploy trained model with FTE-supported prediction server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKvfVP62RM6Y"
      },
      "outputs": [],
      "source": [
        "model = aiplatform.Model(get_model_name(custom_job_id))\n",
        "endpoint = model.deploy(machine_type=\"n1-standard-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnBIG18m6sFM"
      },
      "source": [
        "### Perform online prediction on raw input\n",
        "\n",
        "Online prediction can be performed on raw input which is specified by a feature map of input columns. The input columns should cover all the input columns as specified in feature transformation configuration. Missing values can be indicated as `\"null\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXYDUbSPSlQ8"
      },
      "outputs": [],
      "source": [
        "# Sample request instances when `auto_transform_config` and all input columns are used:\n",
        "# instances = [\n",
        "#   {\"id\": \"1084\", \"ps_ind_01\": \"3\", \"ps_ind_02_cat\": \"1\", \"ps_ind_03\": \"3\", \"ps_ind_04_cat\": \"1\", \"ps_ind_05_cat\": \"0\", \"ps_ind_06_bin\": \"0\", \"ps_ind_07_bin\": \"0\", \"ps_ind_08_bin\": \"0\", \"ps_ind_09_bin\": \"1\", \"ps_ind_10_bin\": \"0\", \"ps_ind_11_bin\": \"0\", \"ps_ind_12_bin\": \"0\", \"ps_ind_13_bin\": \"0\", \"ps_ind_14\": \"0\", \"ps_ind_15\": \"4\", \"ps_ind_16_bin\": \"1\", \"ps_ind_17_bin\": \"0\", \"ps_ind_18_bin\": \"0\", \"ps_reg_01\": \"0.1\", \"ps_reg_02\": \"0.2\", \"ps_reg_03\": \"-1.0\", \"ps_car_01_cat\": \"4\", \"ps_car_02_cat\": \"1\", \"ps_car_03_cat\": \"-1\", \"ps_car_04_cat\": \"0\", \"ps_car_05_cat\": \"0\", \"ps_car_06_cat\": \"14\", \"ps_car_07_cat\": \"1\", \"ps_car_08_cat\": \"1\", \"ps_car_09_cat\": \"0\", \"ps_car_10_cat\": \"1\", \"ps_car_11_cat\": \"68\", \"ps_car_11\": \"3\", \"ps_car_12\": \"0.4\", \"ps_car_13\": \"0.936407766\", \"ps_car_14\": \"0.4074309757\", \"ps_car_15\": \"3.4641016150999997\", \"ps_calc_01\": \"0.6\", \"ps_calc_02\": \"0.1\", \"ps_calc_03\": \"0.3\", \"ps_calc_04\": \"1\", \"ps_calc_05\": \"1\", \"ps_calc_06\": \"9\", \"ps_calc_07\": \"3\", \"ps_calc_08\": \"10\", \"ps_calc_09\": \"3\", \"ps_calc_10\": \"8\", \"ps_calc_11\": \"4\", \"ps_calc_12\": \"2\", \"ps_calc_13\": \"2\", \"ps_calc_14\": \"4\", \"ps_calc_15_bin\": \"1\", \"ps_calc_16_bin\": \"0\", \"ps_calc_17_bin\": \"1\", \"ps_calc_18_bin\": \"0\", \"ps_calc_19_bin\": \"1\", \"ps_calc_20_bin\": \"0\"},\n",
        "#   {\"id\": \"982\", \"ps_ind_01\": \"2\", \"ps_ind_02_cat\": \"2\", \"ps_ind_03\": \"2\", \"ps_ind_04_cat\": \"1\", \"ps_ind_05_cat\": \"0\", \"ps_ind_06_bin\": \"0\", \"ps_ind_07_bin\": \"0\", \"ps_ind_08_bin\": \"0\", \"ps_ind_09_bin\": \"1\", \"ps_ind_10_bin\": \"0\", \"ps_ind_11_bin\": \"0\", \"ps_ind_12_bin\": \"0\", \"ps_ind_13_bin\": \"0\", \"ps_ind_14\": \"0\", \"ps_ind_15\": \"6\", \"ps_ind_16_bin\": \"0\", \"ps_ind_17_bin\": \"1\", \"ps_ind_18_bin\": \"0\", \"ps_reg_01\": \"0.9\", \"ps_reg_02\": \"0.7\", \"ps_reg_03\": \"0.9013878189\", \"ps_car_01_cat\": \"6\", \"ps_car_02_cat\": \"1\", \"ps_car_03_cat\": \"-1\", \"ps_car_04_cat\": \"0\", \"ps_car_05_cat\": \"0\", \"ps_car_06_cat\": \"15\", \"ps_car_07_cat\": \"1\", \"ps_car_08_cat\": \"0\", \"ps_car_09_cat\": \"0\", \"ps_car_10_cat\": \"1\", \"ps_car_11_cat\": \"97\", \"ps_car_11\": \"2\", \"ps_car_12\": \"0.3605551275\", \"ps_car_13\": \"0.8865151386\", \"ps_car_14\": \"-1.0\", \"ps_car_15\": \"3.6055512755000003\", \"ps_calc_01\": \"0.4\", \"ps_calc_02\": \"0.7\", \"ps_calc_03\": \"0.5\", \"ps_calc_04\": \"1\", \"ps_calc_05\": \"1\", \"ps_calc_06\": \"9\", \"ps_calc_07\": \"4\", \"ps_calc_08\": \"8\", \"ps_calc_09\": \"2\", \"ps_calc_10\": \"7\", \"ps_calc_11\": \"5\", \"ps_calc_12\": \"0\", \"ps_calc_13\": \"5\", \"ps_calc_14\": \"8\", \"ps_calc_15_bin\": \"0\", \"ps_calc_16_bin\": \"1\", \"ps_calc_17_bin\": \"0\", \"ps_calc_18_bin\": \"0\", \"ps_calc_19_bin\": \"0\", \"ps_calc_20_bin\": \"0\"},\n",
        "#   {\"id\": \"753\", \"ps_ind_01\": \"1\", \"ps_ind_02_cat\": \"1\", \"ps_ind_03\": \"5\", \"ps_ind_04_cat\": \"0\", \"ps_ind_05_cat\": \"0\", \"ps_ind_06_bin\": \"1\", \"ps_ind_07_bin\": \"0\", \"ps_ind_08_bin\": \"0\", \"ps_ind_09_bin\": \"0\", \"ps_ind_10_bin\": \"0\", \"ps_ind_11_bin\": \"0\", \"ps_ind_12_bin\": \"0\", \"ps_ind_13_bin\": \"0\", \"ps_ind_14\": \"0\", \"ps_ind_15\": \"5\", \"ps_ind_16_bin\": \"1\", \"ps_ind_17_bin\": \"0\", \"ps_ind_18_bin\": \"0\", \"ps_reg_01\": \"0.5\", \"ps_reg_02\": \"0.3\", \"ps_reg_03\": \"0.7088723439\", \"ps_car_01_cat\": \"7\", \"ps_car_02_cat\": \"1\", \"ps_car_03_cat\": \"-1\", \"ps_car_04_cat\": \"0\", \"ps_car_05_cat\": \"-1\", \"ps_car_06_cat\": \"0\", \"ps_car_07_cat\": \"1\", \"ps_car_08_cat\": \"1\", \"ps_car_09_cat\": \"2\", \"ps_car_10_cat\": \"1\", \"ps_car_11_cat\": \"32\", \"ps_car_11\": \"3\", \"ps_car_12\": \"0.316227766\", \"ps_car_13\": \"0.6698649179\", \"ps_car_14\": \"0.3615245497\", \"ps_car_15\": \"3.3166247904\", \"ps_calc_01\": \"0.9\", \"ps_calc_02\": \"0.6\", \"ps_calc_03\": \"0.2\", \"ps_calc_04\": \"2\", \"ps_calc_05\": \"3\", \"ps_calc_06\": \"9\", \"ps_calc_07\": \"3\", \"ps_calc_08\": \"10\", \"ps_calc_09\": \"3\", \"ps_calc_10\": \"12\", \"ps_calc_11\": \"5\", \"ps_calc_12\": \"3\", \"ps_calc_13\": \"2\", \"ps_calc_14\": \"6\", \"ps_calc_15_bin\": \"0\", \"ps_calc_16_bin\": \"1\", \"ps_calc_17_bin\": \"1\", \"ps_calc_18_bin\": \"1\", \"ps_calc_19_bin\": \"0\", \"ps_calc_20_bin\": \"0\"}\n",
        "# ]\n",
        "\n",
        "# Sample request instances when other feature transform configurations and a subset of input columns are used:\n",
        "instances=[\n",
        "    {\"ps_reg_01\": \"0.5\", \"ps_reg_02\": \"0.5\", \"ps_reg_03\": \"0.6\", \"ps_ind_10_bin\": \"1\", \"ps_ind_11_bin\": \"1\", \"ps_ind_12_bin\": \"0\"},\n",
        "    {\"ps_reg_01\": \"0.5\", \"ps_reg_02\": \"0.5\", \"ps_reg_03\": \"0.6\", \"ps_ind_10_bin\": \"1\", \"ps_ind_11_bin\": \"1\", \"ps_ind_12_bin\": \"null\"},\n",
        "    {\"ps_reg_01\": \"null\", \"ps_reg_02\": \"0.5\", \"ps_reg_03\": \"0.6\", \"ps_ind_10_bin\": \"1\", \"ps_ind_11_bin\": \"1\", \"ps_ind_12_bin\": \"0\"}\n",
        "]\n",
        "\n",
        "endpoint.predict(instances=instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZmfLEp8S6N5"
      },
      "source": [
        "### Perform batch prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzEomK10LKSb"
      },
      "outputs": [],
      "source": [
        "write_instances_to_jsonl(instances, os.path.join(root_dir, \"bp_input.jsonl\"))\n",
        "\n",
        "batch_predict_job = model.batch_predict(\n",
        "    job_display_name=\"bp_\" + custom_job_id,\n",
        "    gcs_source=os.path.join(root_dir, \"bp_input.jsonl\"),\n",
        "    gcs_destination_prefix=root_dir,\n",
        "    instances_format=\"jsonl\",\n",
        "    predictions_format=\"jsonl\",\n",
        "    model_parameters=None,\n",
        "    machine_type=\"n1-standard-8\",\n",
        "    starting_replica_count=1,\n",
        "    max_replica_count=1,\n",
        "    sync=False,\n",
        ")\n",
        "\n",
        "batch_predict_job.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHWDXD6mw4h8"
      },
      "source": [
        "## Clean-up resources\n",
        "\n",
        "Clean up all Google Cloud resources used in this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TEKgiFBWxf3"
      },
      "outputs": [],
      "source": [
        "endpoint.undeploy_all()\n",
        "endpoint.delete()\n",
        "model.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "[Private Preview] Feature Transform Engine with XGBoost & SKLearn Trainer Pipelines",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}